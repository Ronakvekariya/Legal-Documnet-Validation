{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open camera.\n",
      "Error: Failed to capture image\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "5\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO(\"./models/NewValidator(1).pt\")\n",
    "\n",
    "# Set up video capture (0 for USB webcam or Raspberry Pi Camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# To hold the data temporarily\n",
    "predictions = []\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Display the current frame\n",
    "    cv2.imshow('Camera Feed - Press \"s\" to simulate IoT signal', frame)\n",
    "    \n",
    "    # Check for keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('s'):  # Simulate IoT signal with the \"s\" key\n",
    "        print(\"IoT Signal Simulated with 's' key: Running YOLOv8 for object detection...\")\n",
    "\n",
    "        # Make prediction on the current frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Check if results contain data\n",
    "        if len(results) > 0:\n",
    "            r = results[0]  # Get the first result\n",
    "            \n",
    "            for box in r.boxes:\n",
    "                # Extract bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # Convert coordinates to integers\n",
    "                confidence = box.conf.cpu().item()  # Confidence score\n",
    "                class_id = int(box.cls.cpu().item())  # Class ID\n",
    "                class_name = r.names[class_id]  # Class name\n",
    "                \n",
    "                # Crop the object from the frame\n",
    "                cropped_object = frame[y1:y2, x1:x2]\n",
    "                \n",
    "                # Save the cropped image\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                cropped_image_path = f\"cropped_objects/{class_name}_{timestamp}.jpg\"\n",
    "                os.makedirs(\"cropped_objects\", exist_ok=True)\n",
    "                cv2.imwrite(cropped_image_path, cropped_object)\n",
    "                \n",
    "                print(f\"Saved cropped object: {cropped_image_path}, Confidence: {confidence}\")\n",
    "\n",
    "                if confidence > 0.96:\n",
    "                    text = f\"{class_name} {confidence:.3f}\"\n",
    "                else:\n",
    "                    text = \"Unknown\"\n",
    "                \n",
    "                image = frame\n",
    "                cv2.rectangle(image , (x1,y1) , (x2,y2) , (0,255,0) , 2)\n",
    "                cv2.putText(image , text ,(x1 , y1-10), cv2.FONT_HERSHEY_COMPLEX , 0.5 ,  (0,144,255) , 1 , cv2.LINE_AA )\n",
    "                cv2.imshow('YOLOv8 Detection', image) \n",
    "\n",
    "    # Press 'q' to quit the program\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoT Signal Simulated with 's' key: Running YOLOv8 for object detection...\n",
      "\n",
      "0: 480x640 3 Infos, 406.0ms\n",
      "Speed: 3.9ms preprocess, 406.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8019])\n",
      "data: tensor([[198.2656, 342.6316, 459.8701, 402.0066,   0.8019,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[329.0679, 372.3191, 261.6045,  59.3750]])\n",
      "xywhn: tensor([[0.5142, 0.7757, 0.4088, 0.1237]])\n",
      "xyxy: tensor([[198.2656, 342.6316, 459.8701, 402.0066]])\n",
      "xyxyn: tensor([[0.3098, 0.7138, 0.7185, 0.8375]])\n",
      "3674 9808 3213\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_1.jpg, Confidence: 0.8018737435340881\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7759])\n",
      "data: tensor([[295.8041, 276.2598, 355.9716, 304.5311,   0.7759,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[325.8878, 290.3954,  60.1675,  28.2714]])\n",
      "xywhn: tensor([[0.5092, 0.6050, 0.0940, 0.0589]])\n",
      "xyxy: tensor([[295.8041, 276.2598, 355.9716, 304.5311]])\n",
      "xyxyn: tensor([[0.4622, 0.5755, 0.5562, 0.6344]])\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_2.jpg, Confidence: 0.775905191898346\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7216])\n",
      "data: tensor([[393.1106, 233.0602, 530.6823, 273.6179,   0.7216,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[461.8965, 253.3390, 137.5717,  40.5577]])\n",
      "xywhn: tensor([[0.7217, 0.5278, 0.2150, 0.0845]])\n",
      "xyxy: tensor([[393.1106, 233.0602, 530.6823, 273.6179]])\n",
      "xyxyn: tensor([[0.6142, 0.4855, 0.8292, 0.5700]])\n",
      "21/08/2004\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_3.jpg, Confidence: 0.7215788960456848\n",
      "IoT Signal Simulated with 's' key: Running YOLOv8 for object detection...\n",
      "\n",
      "0: 480x640 3 Infos, 416.7ms\n",
      "Speed: 6.0ms preprocess, 416.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7811])\n",
      "data: tensor([[220.5180, 383.3895, 507.2290, 439.9331,   0.7811,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[363.8735, 411.6613, 286.7110,  56.5436]])\n",
      "xywhn: tensor([[0.5686, 0.8576, 0.4480, 0.1178]])\n",
      "xyxy: tensor([[220.5180, 383.3895, 507.2290, 439.9331]])\n",
      "xyxyn: tensor([[0.3446, 0.7987, 0.7925, 0.9165]])\n",
      "3674 9808 3213\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_4.jpg, Confidence: 0.7810505032539368\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7736])\n",
      "data: tensor([[329.2474, 303.2461, 397.5262, 333.3878,   0.7736,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[363.3868, 318.3170,  68.2787,  30.1417]])\n",
      "xywhn: tensor([[0.5678, 0.6632, 0.1067, 0.0628]])\n",
      "xyxy: tensor([[329.2474, 303.2461, 397.5262, 333.3878]])\n",
      "xyxyn: tensor([[0.5144, 0.6318, 0.6211, 0.6946]])\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_5.jpg, Confidence: 0.7735639810562134\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7324])\n",
      "data: tensor([[440.2107, 263.7452, 590.1863, 299.6530,   0.7324,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[515.1985, 281.6991, 149.9756,  35.9077]])\n",
      "xywhn: tensor([[0.8050, 0.5869, 0.2343, 0.0748]])\n",
      "xyxy: tensor([[440.2107, 263.7452, 590.1863, 299.6530]])\n",
      "xyxyn: tensor([[0.6878, 0.5495, 0.9222, 0.6243]])\n",
      "21/08/2004\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_6.jpg, Confidence: 0.732414960861206\n",
      "IoT Signal Simulated with 's' key: Running YOLOv8 for object detection...\n",
      "\n",
      "0: 480x640 3 Infos, 402.3ms\n",
      "Speed: 4.0ms preprocess, 402.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7989])\n",
      "data: tensor([[173.5773, 373.0461, 477.4503, 437.5493,   0.7989,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[325.5138, 405.2977, 303.8730,  64.5032]])\n",
      "xywhn: tensor([[0.5086, 0.8444, 0.4748, 0.1344]])\n",
      "xyxy: tensor([[173.5773, 373.0461, 477.4503, 437.5493]])\n",
      "xyxyn: tensor([[0.2712, 0.7772, 0.7460, 0.9116]])\n",
      "3674 9808 3212\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_7.jpg, Confidence: 0.7988945245742798\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7833])\n",
      "data: tensor([[409.2195, 250.6567, 580.6841, 293.3339,   0.7833,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[494.9518, 271.9953, 171.4645,  42.6771]])\n",
      "xywhn: tensor([[0.7734, 0.5667, 0.2679, 0.0889]])\n",
      "xyxy: tensor([[409.2195, 250.6567, 580.6841, 293.3339]])\n",
      "xyxyn: tensor([[0.6394, 0.5222, 0.9073, 0.6111]])\n",
      "21/08/2004\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_8.jpg, Confidence: 0.7833024263381958\n",
      "bouding box ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.7800])\n",
      "data: tensor([[289.2393, 290.0796, 361.1864, 321.6393,   0.7800,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (480, 640)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[325.2129, 305.8594,  71.9470,  31.5597]])\n",
      "xywhn: tensor([[0.5081, 0.6372, 0.1124, 0.0657]])\n",
      "xyxy: tensor([[289.2393, 290.0796, 361.1864, 321.6393]])\n",
      "xyxyn: tensor([[0.4519, 0.6043, 0.5644, 0.6701]])\n",
      "\n",
      "Saved cropped object: cropped_objects/Info_9.jpg, Confidence: 0.7799543738365173\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = YOLO(\"./models/best (1).pt\")\n",
    "\n",
    "# Set up video capture (0 for USB webcam or Raspberry Pi Camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# To hold the data temporarily\n",
    "predictions = []\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Display the current frame\n",
    "    cv2.imshow('Camera Feed - Press \"s\" to simulate IoT signal', frame)\n",
    "    \n",
    "    # Check for keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('s'):  # Simulate IoT signal with the \"s\" key\n",
    "        print(\"IoT Signal Simulated with 's' key: Running YOLOv8 for object detection...\")\n",
    "\n",
    "        # Make prediction on the current frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Check if results contain data\n",
    "        if len(results) > 0:\n",
    "            r = results[0]  # Get the first result\n",
    "            \n",
    "            for box in r.boxes:\n",
    "                print(\"bouding box\",box)\n",
    "                # Extract bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # Convert coordinates to integers\n",
    "                confidence = box.conf.cpu().item()  # Confidence score\n",
    "                class_id = int(box.cls.cpu().item())  # Class ID\n",
    "                class_name = r.names[class_id]  # Class name\n",
    "                \n",
    "                # Crop the object from the frame\n",
    "                cropped_object = frame[y1:y2, x1:x2]\n",
    "                \n",
    "                # Save the cropped image\n",
    "                counter += 1\n",
    "                cropped_image_path = f\"cropped_objects/{class_name}_{counter}.jpg\"\n",
    "                os.makedirs(\"cropped_objects\", exist_ok=True)\n",
    "                cv2.imwrite(cropped_image_path, cropped_object)\n",
    "                image = cv2.imread(cropped_image_path)\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                sharpened = cv2.GaussianBlur(gray, (0, 0), 3)\n",
    "                sharpened = cv2.addWeighted(gray, 1.5, sharpened, -0.5, 0)\n",
    "                text = pytesseract.image_to_string(sharpened)\n",
    "                print(text)\n",
    "                \n",
    "                print(f\"Saved cropped object: {cropped_image_path}, Confidence: {confidence}\")\n",
    "                \n",
    "                # Annotate and display the frame\n",
    "                annotated_frame = results[0].plot()\n",
    "                cv2.imshow('YOLOv8 Detection', annotated_frame) \n",
    "\n",
    "    # Press 'q' to quit the program\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sharpened = cv2.GaussianBlur(gray, (0, 0), 3)\n",
    "    sharpened = cv2.addWeighted(gray, 1.5, sharpened, -0.5, 0)\n",
    "    text1 = pytesseract.image_to_string(sharpened)\n",
    "    \n",
    "\n",
    "    # Step 2: Convert to Grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 3: Apply GaussianBlur to Reduce Noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Step 4: Apply Thresholding\n",
    "    # You can experiment with different thresholding techniques\n",
    "    # Binary Thresholding\n",
    "    # _, binary_image = cv2.threshold(blurred, 150, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    # Adaptive Thresholding\n",
    "    adaptive_threshold = cv2.adaptiveThreshold(blurred, 255, \n",
    "                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                            cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Step 5: Sharpen the Image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "    sharpened = cv2.morphologyEx(adaptive_threshold, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Step 6: Resize the Image\n",
    "    resized_image = cv2.resize(sharpened, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    custom_config = r'--oem 3 --psm 6'  # OCR Engine Mode 3, Page Segmentation Mode 6\n",
    "    text2 = pytesseract.image_to_string(resized_image, config=custom_config)\n",
    "    \n",
    "    return text1,text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 Aadhar Card, 361.0ms\n",
      "Speed: 2.0ms preprocess, 361.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parse_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(cropped_image_path, cv2\u001b[38;5;241m.\u001b[39mcvtColor(cropped_object, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Read the cropped image using OpenCV\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m TEXT1,TEXT2 \u001b[38;5;241m=\u001b[39m \u001b[43mparse_image\u001b[49m(cropped_image_path)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cropped object: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcropped_image_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEXT1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEXT2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parse_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the image and mode\n",
    "image_path = \"./new_aadhar_card_ocr/train/images/0c0584201ff552c4bdcbe160315aa432_jpg.rf.96da227a111410ee00fc2f6ef73ba432.jpg\"\n",
    "model_path = \"./models/NewValidator(1).pt\"\n",
    "image = Image.open(image_path)\n",
    "model = YOLO(model_path)\n",
    "results = model(image)\n",
    "counter = 0\n",
    "image = np.array(image)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('./cropped_objects', exist_ok=True)\n",
    "\n",
    "# Check if results contain data\n",
    "if len(results) > 0:\n",
    "    r = results[0]  # Get the first result\n",
    "\n",
    "    for box in r.boxes:\n",
    "        # Extract bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())  # Convert coordinates to integers\n",
    "        confidence = box.conf.cpu().item()  # Confidence score\n",
    "        class_id = int(box.cls.cpu().item())  # Class ID\n",
    "        class_name = r.names[class_id]  # Class name\n",
    "\n",
    "        # Check if the bounding box is within the image bounds\n",
    "        if x1 >= 0 and y1 >= 0 and x2 <= image.shape[1] and y2 <= image.shape[0]:\n",
    "            # Crop the object from the frame\n",
    "            cropped_object = image[y1:y2, x1:x2]\n",
    "\n",
    "            # Check if the cropped object is not empty\n",
    "            if cropped_object.size > 0:\n",
    "                # Save the cropped image with a unique filename\n",
    "                counter += 1\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                unique_filename = f\"{class_name}_{counter}_{timestamp}.jpg\"\n",
    "                cropped_image_path = os.path.join('./cropped_objects', unique_filename)\n",
    "                cv2.imwrite(cropped_image_path, cv2.cvtColor(cropped_object, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "                # Read the cropped image using OpenCV\n",
    "                TEXT1,TEXT2 = parse_image(cropped_image_path)\n",
    "\n",
    "                print(f\"Saved cropped object: {cropped_image_path}, Confidence: {confidence}\")\n",
    "                print(f\"Extracted Text: {TEXT1}  {TEXT2}\")\n",
    "            else:\n",
    "                print(f\"Skipping empty cropped object for class: {class_name}\")\n",
    "        else:\n",
    "            print(f\"Bounding box out of image bounds for class: {class_name}\")\n",
    "else:\n",
    "    print(\"No objects detected in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easyocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEasyOCR installed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'easyocr'"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['en'])\n",
    "print(\"EasyOCR installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 10 Result, 493.6ms\n",
      "Speed: 4.0ms preprocess, 493.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "10 Result: 0.94\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./10th_marksheet_entity_detection.v3i.yolov11/train/images/10th_marksheet_4_jpg.rf.55295fc9a8e0ca7f5a6f5c39afaa24df.jpg\"\n",
    "model_path = \"./models/results.pt\"\n",
    "\n",
    "\n",
    "# Load the YOLOv8 model (you can replace 'yolov8n.pt' with another model like 'yolov8s.pt' or 'yolov8m.pt')\n",
    "model = YOLO(model_path)\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "results = model(image)\n",
    "\n",
    "# Process results\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        # Get bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  \n",
    "\n",
    "        # Get confidence score\n",
    "        confidence = box.conf[0].item()\n",
    "\n",
    "        # Get class name\n",
    "        class_id = int(box.cls[0].item())\n",
    "        class_name = model.names[class_id]  # Get class name from YOLO model\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Put class name & confidence score on the image\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        print(label)\n",
    "\n",
    "# Show the output image\n",
    "cv2.imshow(\"YOLO Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 200)\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('./ronakaadha.jpg')\n",
    "\n",
    "# Method 1: Resize to specific size\n",
    "new_size = (300, 200)  # width, height\n",
    "resized_image = image.resize(new_size)\n",
    "print(resized_image.size)  # Output: (300, 200)\n",
    "resized_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "document_validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
